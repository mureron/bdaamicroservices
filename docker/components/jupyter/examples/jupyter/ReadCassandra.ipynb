{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Descripción del escenario.\n",
    "\n",
    "El siguiente ejemplo carga dos tablas de Cassandra en Spark y luegos las procesa mediante SparkSQL \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se Cargan las librerias\n",
    "\n",
    "se utilizan la libreria pyspark.sql la cual permite importat datos y cargarlos en dataframe de Spark\n",
    "\n",
    "pyspark.sql.function: Incluye las bibliotecas necesarias para la carga de dataframe. \n",
    "pyspark.sql.types: Lista de tipos de Datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import UserDefinedFunction, lit, explode, desc, expr\n",
    "from pyspark.sql.types import StringType, ArrayType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de la tabla movies desde Cassandra \n",
    "\n",
    "Se cargan los elementos de la tabla movies que se utilizaron en el ejemplo anterior (02_InsertCassandra). Luego se le notifica a Spark que hará una lectura de Cassandra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "\n",
    "# Se seleccionan las opciones que va a leer en Cassandra, ejemplo Table, la clave y luego se define la división \n",
    "# de los datos. \n",
    "\n",
    "load_options = { \"table\": \"movies\", \"keyspace\": \"test\", \"spark.cassandra.input.split.size_in_mb\": \"10\"}\n",
    "dfMovies = spark.read.format(\"org.apache.spark.sql.cassandra\").options(**load_options).load()\n",
    "dfMovies.show()\n",
    "fin = time.time()\n",
    "\n",
    "print(\"Tiempo: \" + str((fin - inicio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de la tabla ratings desde Cassandra\n",
    "\n",
    "De manera similar se ha cargado la tabla de Ratings que se tiene almacenada en Cassandra y que se calculo en el ejemplo anterior (02_InsertCassandra) y se almacenan en memoria en la variable dfRating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "\n",
    "# Se seleccionan las opciones que va a leer en Cassandra, ejemplo Table, la clave y luego se define la división \n",
    "# de los datos. \n",
    "\n",
    "load_options = { \"table\": \"ratings\", \"keyspace\": \"test\", \"spark.cassandra.input.split.size_in_mb\": \"10\"}\n",
    "dfRatings = spark.read.format(\"org.apache.spark.sql.cassandra\").options(**load_options).load()\n",
    "fin = time.time()\n",
    "dfRatings.show()\n",
    "\n",
    "print(\"Tiempo: \" + str((fin - inicio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobamos el número de registros cargados\n",
    "\n",
    "Luego contamos el número de registro que se han extraido de Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Movies: \" + str(dfMovies.count()))\n",
    "print(\"Ratings: \" + str(dfRatings.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de los datos\n",
    "\n",
    "Obtenemos las 100 películas más populares\n",
    "\n",
    "Por cada usuario, se añaden columnas con las 100 películas más populares y la puntuación que le ha dado cada uno, en las columnas que no tienen puntuación para las películas aparecen como NULL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "\n",
    "# Se realiza un agrupación de los datos por movie_id,  luego se cuentan por cada id. Se utiliza el sort para ordenarla\n",
    "# y luego limita a las primeras 100 y finalmete obtiene un collect de todos los workers\n",
    "\n",
    "\n",
    "popular = dfRatings.groupBy(\"movie_id\").count().sort(desc(\"count\")).limit(150).rdd.map(lambda r : r.movie_id).collect()\n",
    "\n",
    "# En este punto se ha realizado un pivot de los datos para las peliculas más populares. \n",
    "\n",
    "ratings_pivot = dfRatings.groupBy(\"user_id\").pivot(\"movie_id\", popular).agg(expr(\"coalesce(first(rating),3)\").cast(\"double\"))\n",
    "ratings_pivot.show()\n",
    "fin = time.time()\n",
    "\n",
    "print(\"Tiempo: \" + str((fin - inicio)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
