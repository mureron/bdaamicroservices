{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de uso de Spark y Jupyter\n",
    "\n",
    "El objetivo del siguiente Notebook es mostrar la integración de Jupyter con Spark en Ecloud. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtener el Contexto de Spark: \n",
    "\n",
    "Para cada ejecución en spark el notebook debe enlazarse a un contexto que retorna el Spark. A continuación podemos visualizar el contexto de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Paralelizar un conjunto de datos: \n",
    "    \n",
    "    Con la función parallelize, el Spark hace una división del conjunto de datos entre los esclavos que integran el clúster de Spark. Luego hace la función collect que devuelve los resultados de los workers. Para el ejemplo se envian10 elementos y se obtienen 10 elementos de los workers.\n",
    "    \n",
    "    Para mayor información se puede visitar [Spark Guide](http://spark.apache.org/docs/2.1.1/programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sc.parallelize(range(10)).collect()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Programar la función PI: \n",
    "Este ejemplo permite demostrar el paralelismo de Spark utilizando los workers para computar el cálculo de PI. En este caso se paraleliza el número de particiones entre 10. Esto permite crear 10 RDD (Resiliance Distributed Dataset), y luego hace el mapeo de los nodos y finalmente hace una operación de reduce para obtener el dato final de PI.\n",
    "\n",
    "Para mayor información sobre RDD visitar [RDD Spark](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "import time\n",
    "\n",
    "partitions = 10\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "inicio = time.time()\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(\"Tiempo: \" + str((fin - inicio)) + \" Sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Ejemplo de PageRank: \n",
    "\n",
    "    En este  ejemplo de Spark  se comienza leyendo  un fichero con un conjunto de URLs e inicializando los vecinos. \n",
    "    Actualiza continuamente las URls consiguiendo una distribución de probabilidad que representa la probabilidad de que una persona haga clic aleatoriamente en vínculos web  y luego lleguen a una página web en particular. Si ejecutamos el programa PageRank con el archivo de datos de entrada e indicamos 10 iteraciones, obtendremos la salida que se presenta al ejecutar el código. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "\n",
    "lines = spark.read.text(\"file:///eslap/legacy/spark/data/mllib/pagerank_data.txt\").rdd.map(lambda r: r[0])\n",
    "\n",
    "# Loads all URLs from input file and initialize their neighbors.\n",
    "\n",
    "links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "\n",
    "# Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "\n",
    "ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "for iteration in range(10):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = links.join(ranks).flatMap(\n",
    "        lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "\n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "    # Collects all URL ranks and dump them to console.\n",
    "    for (link, rank) in ranks.collect():\n",
    "        print(\"%s has rank: %s.\" % (link, rank))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
